{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ6DQiUwALbXLNtGzEaVOs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saishmudliyar/DS-Experiment/blob/main/Experiment_9%2610_Saish_Mudliyar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 9 & 10\n",
        "Apply wrapper feature selection Techniques Part-1 & 2"
      ],
      "metadata": {
        "id": "MUsQzKMwzifF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward selection"
      ],
      "metadata": {
        "id": "uromvJoTxxXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27n3XvQDxgp0",
        "outputId": "0577440c-e0e8-4b05-df49-e94ace4b9804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: [2, 3, 0, 1]\n",
            "Model accuracy with selected features: 0.9777777777777777\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "model = GaussianNB()\n",
        "\n",
        "def forward_selection(X, y, model):\n",
        "    selected_features = []\n",
        "    remaining_features = list(range(X.shape[1]))\n",
        "\n",
        "    # Loop until all features are evaluated\n",
        "    while remaining_features:\n",
        "        best_acc = 0\n",
        "        best_feature = None\n",
        "\n",
        "        # Try adding each remaining feature and evaluate performance\n",
        "        for feature in remaining_features:\n",
        "            X_selected = X[:, selected_features + [feature]]\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            # Track the best feature that improves accuracy\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                best_feature = feature\n",
        "\n",
        "        # If a feature improves accuracy, add it to the selected features\n",
        "        if best_feature is not None:\n",
        "            selected_features.append(best_feature)\n",
        "            remaining_features.remove(best_feature)\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "# Perform Forward Selection\n",
        "selected_features = forward_selection(X, y, model)\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "# Evaluate the model with selected features\n",
        "X_selected = X[:, selected_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model accuracy with selected features:\", model.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward elimination"
      ],
      "metadata": {
        "id": "-M0WzdJ_yMiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "model = GaussianNB()\n",
        "\n",
        "def backward_elimination(X, y, model):\n",
        "    selected_features = list(range(X.shape[1]))  # All features initially\n",
        "\n",
        "    # Loop until all features are evaluated\n",
        "    while len(selected_features) > 1:\n",
        "        best_acc = 0\n",
        "        worst_feature = None\n",
        "\n",
        "        # Try removing each feature and evaluate performance\n",
        "        for feature in selected_features:\n",
        "            selected_features_temp = [f for f in selected_features if f != feature]\n",
        "            X_selected = X[:, selected_features_temp]\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            # Track the worst feature (the one whose removal improves accuracy)\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                worst_feature = feature\n",
        "\n",
        "        # If removing a feature improves accuracy, remove it\n",
        "        if worst_feature is not None:\n",
        "            selected_features.remove(worst_feature)\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "# Perform Backward Elimination\n",
        "selected_features = backward_elimination(X, y, model)\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "# Evaluate the model with selected features\n",
        "X_selected = X[:, selected_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model accuracy with selected features:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwidc3V4yMnT",
        "outputId": "f44c328b-da66-4d82-8a39-d4b60da59bdb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: [3]\n",
            "Model accuracy with selected features: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFE"
      ],
      "metadata": {
        "id": "JP8kvInhzBky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the Logistic Regression model (since it has coef_ for feature importance)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "def rfe_feature_selection(X, y, model):\n",
        "    # Perform RFE (Recursive Feature Elimination) for feature selection\n",
        "    rfe = RFE(estimator=model, n_features_to_select=1, step=1)\n",
        "    rfe = rfe.fit(X, y)\n",
        "\n",
        "    # Get the selected features (those with ranking 1)\n",
        "    selected_features = [i for i in range(X.shape[1]) if rfe.support_[i]]\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "# Perform RFE for feature selection\n",
        "selected_features = rfe_feature_selection(X, y, model)\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "# Evaluate the model with selected features\n",
        "X_selected = X[:, selected_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model accuracy with selected features:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vsjSKDszBot",
        "outputId": "74f23871-570a-4815-ce1d-60dd9691cf6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: [2]\n",
            "Model accuracy with selected features: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFE CV"
      ],
      "metadata": {
        "id": "PyZdRonizINr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the Logistic Regression model (since it has coef_ for feature importance)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "def rfecv_feature_selection(X, y, model):\n",
        "    # Perform RFE with Cross-Validation (RFE-CV) for feature selection\n",
        "    rfecv = RFECV(estimator=model, step=1, cv=5)\n",
        "    rfecv.fit(X, y)\n",
        "\n",
        "    # Get the selected features (those with ranking 1)\n",
        "    selected_features = [i for i in range(X.shape[1]) if rfecv.support_[i]]\n",
        "\n",
        "    return selected_features, rfecv\n",
        "\n",
        "# Perform RFE-CV for feature selection\n",
        "selected_features, rfecv = rfecv_feature_selection(X, y, model)\n",
        "print(\"Selected features:\", selected_features)\n",
        "print(\"Optimal number of features:\", rfecv.n_features_)\n",
        "\n",
        "# Evaluate the model with selected features\n",
        "X_selected = X[:, selected_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model accuracy with selected features:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9qpqhzCzITt",
        "outputId": "b8234a06-ea74-4e99-9a35-cc3df57936fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: [0, 1, 2, 3]\n",
            "Optimal number of features: 4\n",
            "Model accuracy with selected features: 1.0\n"
          ]
        }
      ]
    }
  ]
}